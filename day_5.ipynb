{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Programming for Scientists - Day 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "In the fifth (and last!) day we will consider performance, i.e. \"how to make my code faster\"!\n",
    "\n",
    "* `multiprocessing`\n",
    "* `multithreading`\n",
    "* `numba` - \"just in time\" compilation of python code\n",
    "* `pybind11` - using external c/c++/fortran code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [0] Parallel Programming\n",
    "\n",
    "One way to make a given task execute faster is to use **parallel programming**, i.e. having more than one worker working on the task at once, reducing the overall time to completion.\n",
    "\n",
    "There are two important concepts: **processes vs threads**:\n",
    "* If you start a program on a computer, its running instance could be called a process.\n",
    "* A given process can start one, or many, threads.\n",
    "* Both processes and threads can be thought of as workers which execute a series of commands (e.g. a block of python code).\n",
    "\n",
    "The main difference is:\n",
    "* Threads (of the same process) run in a shared memory space, i.e. they can easily share variables and memory.\n",
    "* Processes run in separate memory spaces: they cannot share variables or memory, unless they explicitly \"communicate\" (i.e. send and receive) information between themselves.\n",
    "\n",
    "You can either use multiple processes, or multiple threads (or both!) to make a program parallel.\n",
    "\n",
    "By definition all threads (of a process) run together on a single computer. On the other hand, multiple processes can run on the same computer (called \"shared-memory parallel\"), or they can actually be spread across different computers (called \"distributed-memory parallel\"), in which case communication must occur across a network.\n",
    "\n",
    "We will focus just on the first case (one computer). If you want to write a program that uses more than one computer, the package to learn is [mpi4py](https://mpi4py.readthedocs.io/en/stable/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [1] Multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A straightforward way to speed up a computation in Python is to have multiple Python processes working together. In this case we use the built-in `multiprocessing` library. This approach is good if:\n",
    "* there is lots of computation to do, and not much data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine we have a function which is very expensive to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to run it for a list of inputs values, and obtain the outputs, we can just do a loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [1,2,3]:\n",
    "    print(f(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three calls to `f()` run **in serial** (one after another). We can instead run them **in parallel**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp.Pool() as p:\n",
    "    args = [1,2,3]\n",
    "    print(p.map(f, args))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We use the `with` syntax, just like opening a file, to let Python automatically clean up all the internal aspects of `Pool` when we are done with it.\n",
    "\n",
    "The `Pool.map` function is a helper function which:\n",
    "* (i) starts a number of independent \"child\" Python processes\n",
    "* (ii) distributes (\"maps\") the set of arguments between these processes\n",
    "* (iii) runs the function `f` with the argument(s) which each process is responsible for\n",
    "* (iv) collects the results by sending them back to the \"parent\" process\n",
    "* (v) shuts down all the child processes\n",
    "\n",
    "Note: Initializing `mp.Pool()` will use all the available CPU cores on the current machine (1 process per core). Instead, `mp.Pool(4)` will start only 4 processes, so these will occupy only 4 cores.\n",
    "\n",
    "> If you are sharing a system (non-exclusive), you will want to avoid using all the CPU cores.\n",
    ">\n",
    "> If you are on a system with only 1 CPU core, there is no point to multiprocessing.\n",
    "\n",
    "Note: the `Pool.starmap` function is similar, except that each element of `args` is extended to also be a list, which is then unpacked and passed to `f` as a list of arguments.\n",
    "\n",
    "> For example, an iterable of `args = [(1,2), (3, 4)]` results in `[func(1,2), func(3,4)]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a convenient helper which abstracts away much of the complexity. For instance, what if we want to run `f()` on 10 different arguments, but only have 4 cores (and so 4 processes)? This is automatically handled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp.Pool(4) as p:\n",
    "    args = np.arange(10)\n",
    "    print(p.map(f, args))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important note to keep in mind: data passed between the processes (the arguments, and the return) is pickled (\"serialized\", using the same `pickle` library we saw earlier to save/load an arbitrary Python object). This is **only ok for small data sizes**, but extremely slow/problematic for large data (e.g. > GB)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it actually faster? Let's make a benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    j = 0\n",
    "    for i in range(10000000):\n",
    "        j += x*x\n",
    "    return j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "args = [1,2,3]\n",
    "for i in args:\n",
    "    print(f(i))\n",
    "    \n",
    "print(f'Took {time.time() - start_time:.2f} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "with mp.Pool() as p:\n",
    "    print(p.map(f, args))\n",
    "    \n",
    "print(f'Took {time.time() - start_time:.2f} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [2] Multithreading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
